{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a039e84b",
   "metadata": {},
   "source": [
    "# Task 1: Symbolic, Unconditioned Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aeb069",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b578b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install miditok\n",
    "# !pip install symusic\n",
    "# !pip install glob\n",
    "# !pip install torch\n",
    "# !pip install pretty_midi\n",
    "# !pip install midi2audio\n",
    "\n",
    "import pretty_midi\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n",
    "import glob\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import GradScaler, autocast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1992bb",
   "metadata": {},
   "source": [
    "### Setup NES-MDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ba34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NESMDB_PATH = \"./nesmdb_midi/\"\n",
    "midi_data = pretty_midi.PrettyMIDI(NESMDB_PATH + 'train/297_SkyKid_00_01StartMusicBGMIntroBGM.mid')\n",
    "\n",
    "for instrument in midi_data.instruments:\n",
    "  print('-' * 80)\n",
    "  print(instrument.name.upper())\n",
    "  print('# note events: {}'.format(len(instrument.notes)))\n",
    "  print('# control change events: {}'.format(len(instrument.control_changes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573632e",
   "metadata": {},
   "source": [
    "### Tokenizer and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be477da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = glob.glob(NESMDB_PATH + \"train/*.mid\")\n",
    "test_files = glob.glob(NESMDB_PATH + \"test/*.mid\")\n",
    "\n",
    "config = TokenizerConfig(\n",
    "    use_time_signatures=True,\n",
    "    use_tempos=True,\n",
    "    use_programs=True,\n",
    "    num_velocities=127,\n",
    "    ac_polyphony_track = True,\n",
    "    ac_polyphony_bar = True,\n",
    ")\n",
    "\n",
    "tokenizer = REMI(config)\n",
    "\n",
    "train_dataset = DatasetMIDI(\n",
    "    files_paths=train_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    "    )\n",
    "test_dataset = DatasetMIDI(\n",
    "    files_paths=test_files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=1024,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd544e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"./nesmdb_midi/train/\"\n",
    "collator = DataCollator(tokenizer.pad_token_id)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collator, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collator, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60739f47",
   "metadata": {},
   "source": [
    "### Model: GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10497d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(MusicGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.norm(self.embedding(x))  \n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ecabce",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a71346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "def train(model, train_loader, val_loader, vocab_size, num_epochs=10, lr=0.001, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.AdamW(model.parameters(), 3e-4, weight_decay=1e-2)\n",
    "    scaler = GradScaler('cuda')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch['input_ids'].to(device)  # (batch_size, seq_length)\n",
    "\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast('cuda'):\n",
    "                outputs, _ = model(inputs)\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                targets = targets.reshape(-1)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch['input_ids'].to(device)\n",
    "\n",
    "                inputs = batch[:, :-1]\n",
    "                targets = batch[:, 1:]\n",
    "\n",
    "                outputs, _ = model(inputs)\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                targets = targets.reshape(-1)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    embedding_dim = 128\n",
    "    hidden_dim = 512\n",
    "    num_layers = 2\n",
    "\n",
    "    model = MusicGRU(vocab_size, embedding_dim, hidden_dim, num_layers)    \n",
    "    train(model, train_loader, test_loader, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a4f9ac",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3361f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, start_token, max_length=100, temperature=0.8, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    generated = [start_token]\n",
    "    input_token = torch.tensor([[start_token]], device=device)  # (1, 1)\n",
    "\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output, hidden = model(input_token, hidden)  # output: (1, 1, vocab_size)\n",
    "        output = output[:, -1, :]  # take the last output\n",
    "        output = output / temperature  # adjust randomness\n",
    "\n",
    "        probs = F.softmax(output, dim=-1)  # (1, vocab_size)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated.append(next_token)\n",
    "        if next_token == 2 or next_token == 0: # reach end of sequence\n",
    "          break\n",
    "\n",
    "        input_token = torch.tensor([[next_token]], device=device)\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = tokenizer.special_tokens_ids[1]\n",
    "generated_sequence = sample(model, start_token, max_length=2048)\n",
    "\n",
    "print(\"Generated token sequence:\")\n",
    "print(generated_sequence)\n",
    "\n",
    "import midi2audio\n",
    "from midi2audio import FluidSynth # Import library\n",
    "from IPython.display import Audio, display\n",
    "fs = FluidSynth(\"FluidR3Mono_GM.sf3\") # Initialize FluidSynth\n",
    "\n",
    "output_score = tokenizer.tokens_to_midi(generated_sequence)\n",
    "print(type(output_score))\n",
    "# boost all note velocities\n",
    "for track in output_score.tracks:\n",
    "    for note in track.notes:\n",
    "        note.velocity = min(127, max(60, int(note.velocity * 2)))\n",
    "\n",
    "output_score.dump_midi(f\"rnn.mid\")\n",
    "fs.midi_to_audio(\"rnn.mid\", \"rnn.wav\")\n",
    "display(Audio(\"rnn.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956bffa7",
   "metadata": {},
   "source": [
    "# Task 2: Continuous, Unconditioned Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb7161",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pyotrch modules\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import lightning as L\n",
    "## basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "## for display purposes\n",
    "from tqdm.auto import trange, tqdm \n",
    "import IPython.display as ipd \n",
    "## for basic tensor operations\n",
    "from einops import rearrange\n",
    "## to read and display wav files\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "## Mamba library\n",
    "from mamba_ssm import Mamba\n",
    "## custom dataloader for the data\n",
    "from dataloader import AudioLoader, extract_waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7716864d",
   "metadata": {},
   "source": [
    "## Model: SeSaMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeSaMe(nn.Module):\n",
    "    def __init__(self,H=1,L=8000):\n",
    "        super().__init__()\n",
    "        ## Linear layers\n",
    "        self.l1 = nn.Linear(4*H, 2*H)\n",
    "        self.l2 = nn.Linear(8*H, 4*H)\n",
    "        self.l3 = nn.Linear(4*H, 8*H)\n",
    "        self.l4 = nn.Linear(2*H, 4*H)\n",
    "        ## Mamba blocks\n",
    "        self.mamba1 = Mamba(d_model= 4*H, ## model dimensionality (i.e dimension of the hidden state)\n",
    "                           d_state=16,\n",
    "                           d_conv=4,\n",
    "                           expand=2\n",
    "                           )\n",
    "        self.mamba2 = Mamba(d_model= 2*H, ## model dimensionality (i.e dimension of the hidden state)\n",
    "                           d_state=16,\n",
    "                           d_conv=4,\n",
    "                           expand=2\n",
    "                           )\n",
    "        self.mamba3 = Mamba(d_model= H, ## model dimensionality (i.e dimension of the hidden state)\n",
    "                           d_state=16,\n",
    "                           d_conv=4,\n",
    "                           expand=2\n",
    "                           )\n",
    "        self.silu = nn.SiLU()\n",
    "        self.relu = nn.GELU()\n",
    "\n",
    "    '''\n",
    "    forward pass of the model (includes skip connections between encoder and decoder)\n",
    "    '''    \n",
    "    def forward(self,x):\n",
    "        x1 = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "        #print(x1.shape)\n",
    "        x11 = x1.reshape(x1.shape[0], x1.shape[1]//4, x1.shape[2]*4)\n",
    "        #print(x11.shape)\n",
    "        x2 = self.relu(self.l1(x11))\n",
    "        #print(x2.shape)\n",
    "        x22 = x2.reshape(x2.shape[0], x2.shape[1]//4, x2.shape[2]*4)\n",
    "        x3 = self.relu(self.l2(x22))\n",
    "        # print(x3.shape)\n",
    "        x4 = x3+self.relu(self.mamba1(x3))\n",
    "        #print(x4.shape)\n",
    "        x5 = self.relu(self.l3(x4))\n",
    "        x55 = x5.reshape(x5.shape[0], x5.shape[1]*4, x5.shape[2]//4)\n",
    "        x6 = self.relu(self.mamba2(x55+x2))\n",
    "        #print(x6.shape)\n",
    "        x7 = self.relu(self.l4(x6))\n",
    "        x77 = x7.reshape(x7.shape[0], x7.shape[1]*4, x7.shape[2]//4)\n",
    "        #print(x77.shape)\n",
    "        y = self.relu(self.mamba3(x77+x1))\n",
    "        y = y.reshape(y.shape[0], y.shape[1])\n",
    "        return y\n",
    "    \n",
    "    '''\n",
    "    encoder part of the model\n",
    "    '''\n",
    "    def encode(self,x):\n",
    "        x1 = x.reshape(x.shape[0], x.shape[1], 1)\n",
    "        #print(x1.shape)\n",
    "        x11 = x1.reshape(x1.shape[0], x1.shape[1]//4, x1.shape[2]*4)\n",
    "        #print(x11.shape)\n",
    "        x2 = self.relu(self.l1(x11))\n",
    "        #print(x2.shape)\n",
    "        x22 = x2.reshape(x2.shape[0], x2.shape[1]//4, x2.shape[2]*4)\n",
    "        x3 = self.relu(self.l2(x22))\n",
    "        return x3\n",
    "    '''\n",
    "    decoder part of the model (w/out skip connections)\n",
    "    '''\n",
    "    def generate(self,x):\n",
    "        x3 = self.relu(self.mamba1(x))\n",
    "        x5 = self.relu(self.l3(x3))\n",
    "        x55 = x5.reshape(x5.shape[0], x5.shape[1]*4, x5.shape[2]//4)\n",
    "        x6 = self.relu(self.mamba2(x55))\n",
    "        #print(x6.shape)\n",
    "        x7 = self.relu(self.l4(x6))\n",
    "        x77 = x7.reshape(x7.shape[0], x7.shape[1]*4, x7.shape[2]//4)\n",
    "        #print(x77.shape)\n",
    "        y = self.relu(self.mamba3(x77))\n",
    "        y = y.reshape(y.shape[0], y.shape[1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80da65d",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aab89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self,model, lr):\n",
    "        self.device = torch.device(\"cuda\") \n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def train(self, train_loader, val_loader, epochs):\n",
    "        nn.init.xavier_uniform_(self.model.l1.weight)\n",
    "        nn.init.normal_(self.model.l1.bias.data)\n",
    "        nn.init.xavier_uniform_(self.model.l2.weight)\n",
    "        nn.init.normal_(self.model.l2.bias.data)\n",
    "        for e in range(epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for x in tqdm(train_loader, desc=f\"Epoch {e+1}\", colour=\"yellow\"):\n",
    "                x = x.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                y = self.model(x)\n",
    "                loss = self.criterion(y,x) ## works only with floating values\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()      \n",
    "            self.evaluate(val_loader, e+1)\n",
    "            print(f\"[Epoch {e+1}] Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        torch.save(self.model.state_dict(), \"sesame_new_weights.pt\")\n",
    "    \n",
    "    def evaluate(self, loader, epoch, setting=\"Validation\"):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        batch_num = 0\n",
    "        with torch.no_grad():\n",
    "            for x in tqdm(loader, desc=f\"Epoch {epoch}\", colour=\"green\"):\n",
    "                x = x.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                y = self.model(x)\n",
    "                loss = self.criterion(y,x) ## works only with floating values\n",
    "                running_loss += loss.item()  \n",
    "                if setting == \"Test\":\n",
    "                    y = y[0,:]\n",
    "                    y = y.cpu().numpy()\n",
    "                    #print(y.shape)\n",
    "                    sf.write(f\"test{batch_num}.wav\",y,16000)\n",
    "                    batch_num+=1\n",
    "            print(f\"[Epoch {epoch}] {setting} Loss: {running_loss/len(loader):.4f}\") \n",
    "    \n",
    "    # def synthesize(self, x,sr=16000):\n",
    "    #     self.model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         x = x.to(self.device)\n",
    "    #         pred = self.model.generate(x)\n",
    "    #         y = pred[0,:]\n",
    "    #         y = y.cpu().numpy()\n",
    "    #         return pred,y\n",
    "            \n",
    "\n",
    "DATAROOT = \"youtube_mix\"\n",
    "loader  = AudioLoader(DATAROOT)\n",
    "model = SeSaMe()      \n",
    "\n",
    "sesame_pipeline = Pipeline(model,1e-4)\n",
    "#sesame_pipeline.train(loader.train_loader, loader.val_loader, 50)\n",
    "sesame_pipeline.model.load_state_dict(torch.load(\"sesame_weights.pt\", weights_only=True))\n",
    "sesame_pipeline.evaluate(loader.test_loader,\"_\",setting=\"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7acedf",
   "metadata": {},
   "source": [
    "### Baseline: Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aabc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This baseline was made with the help of generative AI\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "from dataloader import AudioLoader\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "MINI_DURATION = 5   \n",
    "DATAROOT = \"youtube_mix\" # Make sure this directory exists and contains your audio files\n",
    "\n",
    "# --- Autoencoder Model Implementation ---\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_channels=[64, 128, 256, 512], kernel_size=4, stride=2):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        layers = []\n",
    "        in_channels = 1 # Audio is typically mono\n",
    "\n",
    "        for out_channels in hidden_channels:\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    # Conv1d often benefits from padding='same' or calculated padding\n",
    "                    # to control output length predictability.\n",
    "                    # With stride=2, kernel_size=4, padding=1 gives output_len = (input_len - 4 + 2)/2 + 1 = (input_len - 2)/2 + 1\n",
    "                    # This approximately halves the dimension.\n",
    "                    nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=1),\n",
    "                    nn.BatchNorm1d(out_channels),\n",
    "                    nn.LeakyReLU(0.2)\n",
    "                )\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Calculate the size of the flattened tensor after convolutional layers\n",
    "        # This is crucial for connecting to the linear layer\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 1, self.input_dim)\n",
    "            dummy_output = self.conv_layers(dummy_input)\n",
    "            self.flat_size = dummy_output.view(1, -1).size(1)\n",
    "\n",
    "        self.fc = nn.Linear(self.flat_size, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add a channel dimension: (batch_size, sequence_length) -> (batch_size, 1, sequence_length)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten\n",
    "        z = self.fc(x)\n",
    "        return z\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, encoder_final_channels, encoder_final_spatial_dim,\n",
    "                 hidden_channels=[512, 256, 128, 64], kernel_size=4, stride=2):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.encoder_final_channels = encoder_final_channels\n",
    "        self.encoder_final_spatial_dim = encoder_final_spatial_dim\n",
    "        self.hidden_channels = hidden_channels # For reference if needed later\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # Linear layer to project latent vector to initial convolutional layer shape\n",
    "        self.fc_in = nn.Linear(latent_dim, self.encoder_final_channels * self.encoder_final_spatial_dim)\n",
    "\n",
    "        layers = []\n",
    "        # The first ConvTranspose1d layer takes encoder_final_channels as input\n",
    "        in_channels = self.encoder_final_channels\n",
    "        \n",
    "        decoder_conv_out_channels = hidden_channels[1:] + [1]\n",
    "\n",
    "\n",
    "        for i, out_channels in enumerate(decoder_conv_out_channels):\n",
    "            # output_padding=1 helps ensure the output length doubles roughly, compensating for padding in Conv1d\n",
    "            is_last_layer = (i == len(decoder_conv_out_channels) - 1)\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=1, output_padding=1),\n",
    "                    nn.BatchNorm1d(out_channels) if not is_last_layer else nn.Identity(), # No BatchNorm on output layer\n",
    "                    nn.LeakyReLU(0.2) if not is_last_layer else nn.Identity(), # No activation on output layer usually for continuous values\n",
    "                    # Using Tanh for audio output, common for -1 to 1 range. If your data is 0-1, use Sigmoid.\n",
    "                    nn.Tanh() if is_last_layer else nn.Identity()\n",
    "                )\n",
    "            )\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv_transpose_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc_in(z)\n",
    "        # Reshape to (batch_size, encoder_final_channels, encoder_final_spatial_dim)\n",
    "        x = x.view(x.size(0), self.encoder_final_channels, self.encoder_final_spatial_dim)\n",
    "        x = self.conv_transpose_layers(x)\n",
    "        # Remove the channel dimension: (batch_size, 1, sequence_length) -> (batch_size, sequence_length)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        # Ensure output matches target_len. ConvTranspose1d can be tricky.\n",
    "        if x.shape[1] > self.output_dim:\n",
    "            x = x[:, :self.output_dim]\n",
    "        elif x.shape[1] < self.output_dim:\n",
    "            pad_len = self.output_dim - x.shape[1]\n",
    "            x = F.pad(x, (0, pad_len))\n",
    "        return x\n",
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        encoder_hidden_channels = [64, 128, 256, 512]\n",
    "        decoder_hidden_channels = [512, 256, 128, 64] # Reverse order of encoder channels\n",
    "\n",
    "        self.encoder = Encoder(input_dim, latent_dim, hidden_channels=encoder_hidden_channels)\n",
    "\n",
    "        # Determine the final shape of the encoder's convolutional output for the decoder's input\n",
    "        with torch.no_grad():\n",
    "            dummy_input_for_encoder_conv = torch.randn(1, 1, input_dim) # (Batch, Channels, Length)\n",
    "            encoder_conv_output = self.encoder.conv_layers(dummy_input_for_encoder_conv)\n",
    "            final_encoder_spatial_dim = encoder_conv_output.shape[-1]\n",
    "            final_encoder_channels = encoder_conv_output.shape[1]\n",
    "\n",
    "        self.decoder = Decoder(latent_dim, input_dim,\n",
    "                               encoder_final_channels=final_encoder_channels,\n",
    "                               encoder_final_spatial_dim=final_encoder_spatial_dim,\n",
    "                               hidden_channels=decoder_hidden_channels) # Pass the explicitly defined channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        recon_x = self(x)\n",
    "        loss = F.mse_loss(recon_x, x) # Standard MSE for Autoencoder\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        recon_x = self(x)\n",
    "        loss = F.mse_loss(recon_x, x)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        recon_x = self(x)\n",
    "        loss = F.mse_loss(recon_x, x)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def generate(self, num_samples, device):\n",
    "        print(\"Warning: Vanilla Autoencoder's 'generate' function samples random points in latent space. \"\n",
    "              \"This is less principled for generating novel samples than a VAE.\")\n",
    "        z = torch.randn(num_samples, self.hparams.latent_dim).to(device) # Random noise\n",
    "        with torch.no_grad():\n",
    "            generated_audio = self.decoder(z)\n",
    "        return generated_audio\n",
    "    \n",
    "audio_loader = AudioLoader(DATAROOT)\n",
    "train_loader = audio_loader.train_loader\n",
    "val_loader = audio_loader.val_loader\n",
    "\n",
    "# Define model parameters\n",
    "INPUT_DIM = SAMPLE_RATE * MINI_DURATION # 16000 * 5 = 80000 samples\n",
    "LATENT_DIM = 128 # This can be tuned\n",
    "\n",
    "# 2. Initialize the Autoencoder model\n",
    "model = Autoencoder(input_dim=INPUT_DIM, latent_dim=LATENT_DIM)\n",
    "\n",
    "# 3. Initialize PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1, # Adjust number of epochs as needed\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1 if torch.cuda.is_available() else 'auto',\n",
    "    logger=True, # Logs to TensorBoard by default\n",
    "    callbacks=[pl.callbacks.ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1)]\n",
    ")\n",
    "\n",
    "# 4. Train the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# --- Generation (from the Autoencoder) ---\n",
    "\n",
    "# Load the best model checkpoint\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "print(f\"Loading best model from: {best_model_path}\")\n",
    "trained_ae = Autoencoder.load_from_checkpoint(best_model_path)\n",
    "trained_ae.eval() # Set to evaluation mode\n",
    "\n",
    "# Move model to the correct device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trained_ae.to(device)\n",
    "\n",
    "# Generate some unconditioned audio samples using the AE's decoder\n",
    "# Note: For a vanilla AE, sampling from pure random noise might not produce good results\n",
    "# if the latent space is not regularized (like in a VAE).\n",
    "# This is mainly for demonstrating the \"generation\" capability.\n",
    "num_samples_to_generate = 5\n",
    "print(f\"Generating {num_samples_to_generate} audio samples using Autoencoder...\")\n",
    "generated_audio_samples = trained_ae.generate(num_samples_to_generate, device)\n",
    "\n",
    "for i, audio_tensor in enumerate(generated_audio_samples):\n",
    "    audio_np = audio_tensor.cpu().numpy()\n",
    "    print(f\"Generated sample {i+1} shape: {audio_np.shape}\")\n",
    "    ipd.display(ipd.Audio(audio_np, rate=SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c066210",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
